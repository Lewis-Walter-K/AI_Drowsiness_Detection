{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79aa3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Tắt cảnh báo cho gọn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923d383",
   "metadata": {},
   "source": [
    "1. Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f77d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Outputs/final.csv\", header=0)\n",
    "print(f\"Original data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a16745",
   "metadata": {},
   "source": [
    "2. Process when No Detection (Face & Pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08129848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop(columns=[\"Image_Path\"])\n",
    "print(f\"Filtered data shape: {df_filtered.shape}\")\n",
    "\n",
    "mask_no_face = (df['FACIAL_DISPLAYED'] == 0)\n",
    "mask_no_pose = (df['POSE_DISPLAYED'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27476e4",
   "metadata": {},
   "source": [
    "Label as Passed_out when cannot detect face / pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.loc[(mask_no_face & mask_no_pose) | (mask_no_face), 'LABEL'] = 3\n",
    "\n",
    "print(f\"Rows labeled as 'No Detection' (Label 3): {df_filtered[df_filtered['LABEL'] == 3].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8bac1",
   "metadata": {},
   "source": [
    "Seperate Feature & Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column\n",
    "X = df.drop(columns=['EYE_CL', 'FACIAL_DISPLAYED', 'POSE_DISPLAYED', 'Image_Path', 'LABEL'])\n",
    "y = df['LABEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa797a72",
   "metadata": {},
   "source": [
    "2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9e69b",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train (70%), Validation (15%), and Test (15%)\n",
    "# Stratify ensures we have examples of Label 3 in all sets\n",
    "X_train_raw, X_temp, y_train_raw, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val_raw, X_test_raw, y_val_raw, y_test_raw = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94baa108",
   "metadata": {},
   "source": [
    "Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# 1. Set up SMOTE\n",
    "# sampling_strategy = 'auto' resamples all classes to match the majority class\n",
    "\n",
    "# 2. Resample X and y\n",
    "\n",
    "print(f\"Original shape: {pd.Series(y_train_raw).value_counts().to_dict()}\")\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train_raw)\n",
    "\n",
    "print(f\"New shape after SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60bf0b",
   "metadata": {},
   "source": [
    "One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19230ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numnber of classes (0: Awake, 1: Sleep, 2: Yawn, 3: Passed_out)\n",
    "num_classes = 4\n",
    "y_train_final = to_categorical(y_train_resampled, num_classes)\n",
    "y_val_final = to_categorical(y_val_raw, num_classes)\n",
    "y_test_final = to_categorical(y_test_raw, num_classes)\n",
    "\n",
    "\n",
    "X_train = X_train_resampled\n",
    "y_train = y_train_final\n",
    "\n",
    "X_val = X_val_scaled\n",
    "y_val = y_val_final\n",
    "\n",
    "X_test = X_test_scaled\n",
    "y_test = y_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8282145",
   "metadata": {},
   "source": [
    "Calculate Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Balancing datasets\n",
    "# # y_integers = np.argmax(y_train, axis = 1)\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes = np.unique(y_integers), y=y_integers)\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "# print(\"Class Weights: \", class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bf65",
   "metadata": {},
   "source": [
    "3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9eaaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(num_nodes, dropout_prob, lr, batch_size, epochs):\n",
    "    model = Sequential([\n",
    "        Dense(num_nodes, activation='relu', input_shape =(X_train.shape[1],)),\n",
    "        BatchNormalization(), # More stable for training\n",
    "        Dropout(dropout_prob),\n",
    "        Dense(num_nodes // 2, activation='relu'), # Second hidden smaller layer\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_prob),\n",
    "        Dense(num_classes, activation='softmax') # Use softmax for multi-class classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall()])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data = (X_val, y_val),\n",
    "        epochs = epochs,\n",
    "        batch_size=batch_size,\n",
    "        #class_weight=class_weights_dict,\n",
    "        callbacks=[early_stop],\n",
    "        verbose = 0 # Silent training\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1527",
   "metadata": {},
   "source": [
    "4. Grid Search (Manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller scale for faster\n",
    "node_counts = [16, 32, 64, 128]\n",
    "dropout_probs = [0.2, 0.4]\n",
    "learning_rates = [0.01, 0.001] # Gold value for Adam, rarely change\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_config = {}\n",
    "\n",
    "total_runs = len(node_counts) * len(dropout_probs) * len(learning_rates) * len(batch_sizes)\n",
    "current_run = 0\n",
    "\n",
    "print(f\"Start GRID SEARCH ({total_runs} combinations...)\")\n",
    "\n",
    "for nodes in node_counts:\n",
    "    for drop in dropout_probs:\n",
    "        for lr in learning_rates:\n",
    "            for batch in batch_sizes:\n",
    "                current_run += 1\n",
    "                print(f\"\\n[{current_run} / {total_runs}] Train: Nodes = {nodes} | Drop = {drop} | LR = {lr} | Batch = {batch}...\", end=\"\")\n",
    "\n",
    "                model, history = build_and_train_model(nodes, drop, lr, batch, epochs = 150)\n",
    "\n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "                print(f\"\\nVal Loss = {val_loss:.4f} | Val Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss=val_loss\n",
    "                    best_model = model # Lưu trực tiếp object model vào RAM\n",
    "                    best_config =   {'nodes': nodes, 'drop': drop, 'lr': lr, 'batch': batch}\n",
    "                    print(f\"Found Best Model | (Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "print(f\"\\nBest Config: {best_config}\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(f\"Training model with best cofig: {best_config}\")\n",
    "\n",
    "# Gọi lại hàm train với các tham số từ best_config\n",
    "final_model, final_history = build_and_train_model(\n",
    "    num_nodes=best_config['nodes'],\n",
    "    dropout_prob=best_config['drop'],\n",
    "    lr=best_config['lr'],\n",
    "    batch_size=best_config['batch'],\n",
    "    epochs=150 # Chạy đủ số epoch để thấy rõ biểu đồ\n",
    ")\n",
    "\n",
    "print(\"Training Done, ploting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077eb197",
   "metadata": {},
   "source": [
    "Ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Lấy dữ liệu từ history\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # --- BIỂU ĐỒ 1: ĐỘ CHÍNH XÁC (ACCURACY) ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', linestyle='--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # --- BIỂU ĐỒ 2: ĐỘ MẤT MÁT (LOSS) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss', linestyle='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Gọi hàm vẽ\n",
    "plot_training_history(final_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874774e0",
   "metadata": {},
   "source": [
    "5. Rate & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating on Test Field\n",
    "loss, acc, recall = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Accuracy on Test Field: {acc*100:.2f}%\")\n",
    "\n",
    "# Save Model\n",
    "save_path = './Outputs/drowsiness_model_best.h5'\n",
    "best_model.save(save_path)\n",
    "print(f\"Best model saved at: {save_path}\")\n",
    "\n",
    "# Save Scaler (for camera using)\n",
    "with open('./Outputs/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Scaler saved at ./Outputs/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c353f96",
   "metadata": {},
   "source": [
    "Predict Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aca053",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = best_model.predict(X_test)\n",
    "\n",
    "#1. Convert Probabilities to Class Lables\n",
    "# y_pred_nn = is likely shape (N,4) with probabilities, we take the index of the highest probabilities\n",
    "y_pred_classes = np.argmax(y_pred_nn, axis=1)\n",
    "\n",
    "# 2. Convert y_test to Class Labels (if it's one-hot-encoded)\n",
    "# if y_test if shape (N,4), we also have argmax\n",
    "if y_test.ndim > 1 and y_test.shape[1] > 1:\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_true = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714724c",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5840bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Print the Classification Report\n",
    "# Define your class names for easier reading\n",
    "\n",
    "class_names = ['Awake (0)', 'Drowsy(1)', 'Yawning (2)', 'Passed out (3)']\n",
    "# Note: Double check your mapping! Usually 0 = Awake, 1 = Drowsy / Closed, 2 = Yawn, 3 = No Face\n",
    "\n",
    "print(f\"\\n----- Classification Report -----\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names = class_names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe9b1",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate and Plot Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.xlabel('Predict Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
